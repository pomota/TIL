## Perceptron
### Neuron
인간의 뉴런 단순화

입력 신호가 역치를 넘으면 다음으로 전파됨

인공신경망

### Perceptron
![image](https://github.com/pomota/TIL/assets/132712212/d5605e1b-938f-45d4-ac7f-79ee4b36bdf5)

W: 가중치, b: bias -> output

output은 activation function을 거침 (sigmoid 등)

초기의 perceptron은 linear classifier를 위해 개발됨

### And, OR
![image](https://github.com/pomota/TIL/assets/132712212/d7b968c7-cd0f-47ce-8049-3d4cc4062a0f)

And, OR: perceptron으로 직선 분류 가능

한 개의 layer를 가진 perceptron은 XOR을 풀 수 없음

여러 layer로 만들 수 있지만, 당시에는 학습 불가능 (현대의 back propagation)

### XOR 예시 코드
```
X = torch.FloatTensor([0,0],[0,1],[1,0],[1,1]).to(device)
Y = torch.FloatTensor([0],[1],[1],[0]).to(device)
# nn layers
linear = torch.nn.Linear(2, 1, bias = True)
sigmoid = torch.nn.Sigmoid()
model = torch.nn.Sequential(linear, sigmoid).to(device)

criterion = torch.nn.BCELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(),lr=1)
for step in range(10001):
  optimizer.zero_grad()
  hypothesis = model(X)
  cost = criterion(hypothesis, Y)
  cost.backward()
  optimizer.step()
  if step % 100 == 0:
    print(step, cost.item())
```

학습 결과: loss가 줄어들지 않음!
