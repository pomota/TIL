## Gradient Descent 심화
- Cost function을 최소화
- 기울기를 따라 내려감

![image](https://github.com/pomota/TIL/assets/132712212/25d3cd43-5860-4c4c-ae81-a62fed97a483)

```
gradient = 2*torch.mean((W*x_train-y_train)*x_train)
lr = 0.1
W -= lr * gradient
```

### 전체 코드
```
x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[4],[5],[6]])

W = torch.zeros(1)
lr = 0.1
nb_epochs = 10

for epoch in range(1, nb_epochs+1):
  hypothesis = W * x_train
  cost = torch.mean((hypothesis-y_train)**2)
  gradient = torch.mean((hypothesis-y_train)*x_train)

  print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), cost.item()))
  
  # d는 정수, f는 소수 (:4d는 총 4자리, .3f는 소수점 아래 3자)
  # 4.3f는 소수점 아래 3자리, 최소 4자리
```

### torch.optim
gradient descent를 위한 라이브러리
1. optimizer 정의: optimizer = optim.SGD([W], lr=0.05)
2. optimizer.zero_grad(): gradient 0으로 초기화
3. cost.backward(): gradient 계산
4. optimizer.step(): gradient descent
