## Multivariate Linear Regression
내용상 차이는 거의 없음 (복습!)
### Hypothesis Function
H(x) = Wx + b (matmul, W와 x는 행렬)

`hypothesis = x_train.matmul(W)+b`

### Cost function: MSE
`cost = torch.mean((hypothesis-y_train)**2)`

### Gradient Descent with torch.optim
```
optimizer = optim.SGD([W,b], lr=1e-5)

optimizer.zero_grad()
cost.backward()
optimizer.set()
```

### nn.Module
```
import torch.nn as nn

class MultivariateLinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(3,1)  # (입력차원, 출력차)

  def forward(self, x):  # Hypothesis 계산하는 함수
    return self.linear(x)

hypothesis = model(x_train)  # Gradient 계산은 pytorch가 알아서 실행 backward()
```

### F.mse_loss
```
import torch.nn.functional as F

cost =F.mse_loss(prediction, y_train)
```

### 전체 코드
```
x_train = torch.FloatTensor(~)
y_train = torch.FloatTensor(~)

model = MultivariateLinearRegressionModel()

optimizer = optim.SGD([W, b], lr = 1e-5)

nb_epoch = 20
for epoch in range(1, nb_epochs+1):
  hypothesis = model(x_train)
  cost = F.mse_loss(prediction, y_train)

  optimizer.zero_grad()
  cost.backward()
  optimizer.step()

  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, hypothesis.squeeze().detach(), cost.item()))
```
