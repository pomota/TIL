## Batch Normalization
### Gradient Vanishing / Exploding
- Gradient Vanishing: Gradient 소멸
- Gradient Exploding: Gradient 너무 커짐

### Solution
- Change activation function
- Careful initialization (Xavier / He 등)
- Small learning rate
- **Batch Normalization** (학습 안정화)

### Internal Covariate Shift
![image](https://github.com/pomota/TIL/assets/132712212/54275bea-1baa-4303-8d66-358af5a77e50)

Covariate shift: train set과 test set의 분포에 차이가 있음

Internal Covariate shift: Layer를 지나면서 Covariate shift (분포 바뀜)


이를 해결하기 위해 Batch Normalization!

### Batch Normalization
Layer마다 normalization을 함 (mini batch마다)

![image](https://github.com/pomota/TIL/assets/132712212/84ddbc0e-0290-40ae-b452-783b5d5e3687)

cf) scale and shift: activation function의 비선형성을 잃는 것을 완화하기 위함

(gamma, beta도 학습을 하는 변수)

### Train/eval mode
```
model.train() # dropout O
model.eval() # dropout X
```

train에서 구한 평균/분산은 batch가 달라지면 바뀜

같은 data에 대해 다른 값을 줄 수 있음

sample mean/variance: 학습 당시에 구하는 평균/분산

learning mean/variance: sample mean/variance를 저장 (입력 데이터의 분포가 아)

-> 학습이 끝나면 입력 batch와 상관없이 고정됨 (실제 inference에서는 이 값으로 normalize)

### MNIS batchnorm
```
linear1 = torch.nn.Linear(784, 32, bias=True)
linear2 = torch.nn.Linear(32, 32, bias=True)
linear3 = torch.nn.Linear(32, 10, bias=True)
relu = torch.nn.ReLU()
bn1 = torch.nn.BatchNorm1d(32)
bn2 = torch.nn.BatchNorm1d(32)

bn_model = torch.nn.sequential(linear1, bn1, relu,~~~)
```
cf) BN 같은 normalization function은 일반적으로 activation function 전에 사용

학습은 동일한 코드
```
for epoch in range(training_epochs):
  bn_model.train() # train mode! (evaluation에서는 bn_model.eval())

  for X, Y in train_loader:
    X = X.view(-1, 28*28).to(device)
    Y = Y.to(device)

    optimizer.zero_grad()
    prediction = bn_model(X)
    loss = criterion(prediction, Y)
    loss.backward()
    optimizer.step()
```
