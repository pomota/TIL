## ReLu
### Sigmoid의 문제점
Loss의 Backpropagation으로 weight update

![image](https://github.com/pomota/TIL/assets/132712212/f2a0de28-b3f1-4f59-9f79-84ee53cd4201)

Vanishing Gradient

gradient가 양끝에서 0에 가까워짐

output과 먼쪽에서 소멸됨

### ReLu
![image](https://github.com/pomota/TIL/assets/132712212/1dd90951-d413-4a16-9c34-2f1eba37ac99)

```
torch.nn.relu(x)
# torch.nn.leaky_relu(x, 0.01)
```

### Optimizer (pytorch)
torch.optim.SGD

torch.optim.Adadelta

torch.optim.Adam

다양한 optimizer가 optim 라이브러리로 제공됨

![image](https://github.com/pomota/TIL/assets/132712212/eaffebfc-c8f6-4fef-b562-908b3d3566d9)

###
```
import torchvision.datasets as dsets
mnist_train = dsets.MNIST(root="MNIST_data/", train=True, transform=transform.ToTensor(), download=True)
mnist_test = dsets.MNIST(root="MNIST_data/", train=False, transform=transform.ToTensor(), download=True)

data_loader = torch.utils.DataLoader(DataLoader=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)

for epoch in range(training_epochs)
  for X, Y in data_loader
    X = X.view(-1, 28*28).to(device)
    
    learning_rate = 0.001
    training_epochs = 15
    batch_size = 100
    
    linear = torch.nn.Linear(784, 10, bias=True).to(device)
    
    torch.nn.init.normal_(linear.weight)
    
    criterion = torch.nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)
```
