## Weight Initialization
학습의 성능에 영향을 줌

Not all 0: gradient 값이 모두 0이 되어 학습 불가

### Restricted Boltzmann Machine (RBM)
![image](https://github.com/pomota/TIL/assets/132712212/39045053-e849-4cff-84a3-580fa29afbd7)

- Restricted: no connections within a layer
- 다른 layer는 fully connected

X -> Y (encoding) Y -> X' (decoding)

### RBM으로 weight initalize
Deep Belief Network

RBM으로 인접한 layer를 pretraining

RBM으로 학습, layer 쌓기 반복

그 후 weight로 fine tuning (forward, backward, 일반적인 학습)

### Xavier / He initialization
RBM은 현재 잘 사용하지 않음

과거에는 무작위 수로 weight initialization

Layer 특성에 따라서 다르게 initialization하는 방법

- Normal initialization
- Uniform initialization

![image](https://github.com/pomota/TIL/assets/132712212/71bdb23c-be56-4cc0-9de7-acd8b25cf3eb)

### Initialization 구현
pytorch 패키지에 있는 함수
```
def xavier_uniform_(tensor, gain=1):
  fan_in, fan_out = _calculate_fan_i_and_fan_out(tensor) # layer의 input, output 수
  std = gain * math.sqrt(2.0/(fan_in+fan_out))
  a = math.sqrt(3.0)*std
  with torch.no_grad():
    return tensor.uniform_(-a, a)
```

### Code
```
torch.nn.init.xavier_uiform_(linear1.weight)
torch.nn.init.xavier_uiform_(linear2.weight)
torch.nn.init.xavier_uiform_(linear3.weight)
```
