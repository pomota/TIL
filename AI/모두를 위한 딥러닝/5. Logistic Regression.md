## Logistic Regression
### Binary classification

X (m*d)에 W를 곱해 -> m열 vector (0, 1로 구성)

W (d*1): weight

W와 X를 곱하고 (W^T X) sigmoid 함수에 대입하여 0, 1로 근사

### Hypothesis
sigmoid function

![image](https://github.com/pomota/TIL/assets/132712212/778eb2d4-a6b7-4a09-b650-1ce58b6ead23)


hypothesis, H(x)

![image](https://github.com/pomota/TIL/assets/132712212/3ad2db79-0150-4748-a0cb-6231a3230005)

H(x) = P(x=1; W) = 1 - P(x=0; W)

(sample이 1일 확률)

### Cost
선형회귀와 같은 MSE를 cost로 사용하면 비선형, 볼록하지 않음 (Gradient descent하기 부적합)

non convex function: local minimum 문제

`Cross Entropy (log loss)`
![image](https://github.com/pomota/TIL/assets/132712212/09f877f8-96b8-4aeb-b0bd-b150b800d563)

### Import
```
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
```

### Training data
```
x_data = [[1,2], [2,3], [3,4], [4,5], [5,6], [6,7]] # x (m,d): (6, 2)
y_data = [[0],[0],[0],[1],[1],[1]] # (6,)

x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)
```

### Hypothesis
```
# exponential: torch.exp()
W = torch.zeros((2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

hypothesis = 1/(1 + torch.exp(-(x_train.matmul(W)+b))) # x W 곱 = torch.matmul(x,w)
# torch.sigmoid(x_train.matmul(W)+b)와 같음
```

### Cost function
```
# Sample 하나에 관한 것
cost = -(y_train[0] * torch.log(hypothesis[0]) + # P(x=1;W)
(1-y_train[0]) * torch.log(1-hypothesis[0])) # P(x=0;W)
```

```
# cost function
losses = -(y_train * torch.log(hypothesis) + (1-y_train) * torch.log(1-hypothesis))
cost = losses.mean()

# F.binary_cross_entropy(hypothesis, y_train)과 같
```

### 전체 training
```
W = torch.zeros((2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
optimizer = optim.SGD([W,b], lr=1)

nb_epochs=1000
for epoch in range(1, nb_epochs+1):
  hypothesis = torch.sigmoid(x_train.matmul(W)+b)
  cost = F.binary_cross_entropy(hypothesis, y_train)
  
  optimizer.zero_grad()
  cost.backward()
  optimizer.step()
  
  if epoch%100==0:
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))
```

### Evaluation
```
hypothesis = torch.sigmoid(x_test.matmul(W)+b)
prediction = hypothesis >= torch.FloatTensor([0.5]) # byteTensor type, 0과 1로 예측
y_test
correct_prediction = prediction.float() == y_test
```

### Class 사용
```
class BinaryClassifier(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(8,1) # self.linear = {W, b}, W (8,1), b (1,)
    self.sigmoid = nn.Sigmoid()
  
  def forward(self, x):
    return self.sigmoid(self.linear(x))
  
model = BinaryClassifier()
```

```
optimizer = optim.SGD(model.paramter(), lr=1)
~~~
hypothesis = model(x_train)
```
