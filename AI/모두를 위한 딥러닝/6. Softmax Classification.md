## Softmax Classification
multiple classification

sample 개수: m

class 개수: d

### Import
```
import torch
import torch.nn as nn
import torch.nn.functional as F
torch.manual_seed(1)
```

### Discrete Probability Distribution
이산확률 분포

ex) 가위바위보, 주사위

### Softmax
max 값을 soft하게 뽑아주는 함수?
Ex)
```
z = torch.FloatTensor([1,2,3])
# 기존의 max = (0, 0, 1)

# Softmax: 합쳐서 1이 되는 값 (확률 )
F.softmax(z, dim=0)
tensor([0.09, 0.2447, 0.6652])
```
모델의 마지막 함수로 softmax를 사용하여 확률분포를 근사

### Cross Entropy
두 개의 확률분포가 얼마나 비슷한지를 나타내는 식
![image](https://github.com/pomota/TIL/assets/132712212/ea4262f4-eee1-4953-afd5-896785a4a051)
![image](https://github.com/pomota/TIL/assets/132712212/cbc2c714-2e4f-4796-8523-b228a46689f7)
![image](https://github.com/pomota/TIL/assets/132712212/93e75a1b-13fc-4868-b76d-c67f0a3c8fc7)

Cross Entropy를 최소화하도록 하면 분포가 가까워질 것임
![image](https://github.com/pomota/TIL/assets/132712212/1e834125-2d26-46f9-aa40-d925468a3461)

### Cross Entropy Loss
```
z = torch.rand(3, 5, requires_grad=True)
y  # z에서 max의 index 값이라고 하자

y_one_hot = torch.zeros_like(hypothesis) # (3,5)의 0 행렬
y_one_hot.scatter_(1, y.unsqueeze(1), 1)
# dimension = 1, y.unsqueeze: (3,) -> (3,5), 1을 대입
# _는 inplace 함수, 메모리 할당 없이 교체됨
# scatter(인덱싱 기준 차원, 인덱스, 변경 값)

cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()
# 각 class에 대해 cross entropy의 시그마의 평
```

### Cross Entropy Loss with torch.nn.functional
```
torch.log(F.softmax(z, dim=1)) # low level
F.log_softmax(z, dim=1) # high level
```

```
# low level
(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()

# high level
F.nll_loss(F.log_softmax(z, dim=1), y) # negative log likelihood

# cross_entropy
F.cross_entropy(z, y)
```

### nn.Module
```
class SoftmaxClassifierModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(4,3) # output이 3 (4->3개의 class의 확률값)
  
  def forward(self, x):
    return self.linear(x) # |x| = (m, 4) -> (m, 3)

model = SoftmaxClassifierModel()
```

### 전체 코드
```
W = torch.zeros((4,3), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([W,b], lr=0.1)

nb_epochs = 1000
for epoch in range(nb_epochs+1):
  hypothesis = F.softmax(x_train.matmul(W)+b, dim=1)

  # low level
  y_one_hot = torch.zeros_like(hypothesis)
  y_one_hot.scatter_(1, y_train.unsqueeze(1), 1) # y_train.unsqueeze(1)은 실제 값이자 인덱스 값
  cost = -(y_one_hot * torch.log(F.softmax(hypothesis, dim=1)).sum(dim=1).mean()

  # high level
  z = x_train.matmul(W)+b
  cost = F.cross_entropy(z, y_train)
  
  optim.zero_grad()
  cost.backward()
  optim.step()
  
  if epoch % 100 == 0:
    print('Epoch {:4d}/{} Cost: {.6f}'.format(epoch, nb_epochs, cost.item()))
```


### nn.module
```
class SoftmaxClassifierModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(4,3)
  
  def forward(self, x):
    return self.linear(x) # |x| = (m, 4) -> (m, 3)

model = SoftmaxClassifierModel()

optimizer = optim.SGD(model.parameters(), lr=0.1)

nb_epochs = 1000
for epoch in range(nb_epochs+1):
  prediction = model(x_train)
  cost = F.cross_entropy(prediction, y_train)

  optimizer.zero_grad()
  cost.backward()
  optimizer.step()
```

binary classification: BCE, sigmoid
multiple classification: CE, softmax
