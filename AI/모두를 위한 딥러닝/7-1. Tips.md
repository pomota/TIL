## Maximum Likelihood Estimation (MLE)
최대 우도 추정

Ex) 동전 던지기: 이항분포 (베르누이 분포)

앞면이 나올 확률 p

관찰값을 토대로 p를 추정하는 것이 모델의 목적!

확률분포 함수 P(X=k)는 p의 함수 f(p)

likelihood: f(p), p에 따른 P의 값

`y값이 최대가 되는 지점 p: maximum likelihood`

observation을 가장 잘 설명하는 p를 찾기: Gradient Descent!!
 

## Overfitting
![image](https://github.com/pomota/TIL/assets/132712212/c0f2046e-646b-4834-8878-d5baf6f46f88)

피하는 방법: 데이터를 나누기

training set / test set / (development set)

(보통 0.8/0.1~0.2/0~0.1 정도 비율로 쪼갬)

development set: test set에 대한 overfitting 방지 (validation set)

![image](https://github.com/pomota/TIL/assets/132712212/2365907e-652b-4ced-a95d-ad3a47e0f229)

overfitting을 피하기 위한 방법
- more data
- less features
- regularization
  
## Regularization
- Early stopping: validation loss가 더 이상 낮아지지 않을 때 학습 중지
- reducing network size
- weight decay
- dropout
- batch normalization

## DNN 학습 과정
1. 모델 설계
2. overfitting 체크: validation set의 loss 높아짐, training set의 loss 낮아짐
- X: 모델 사이즈 키우기 (layer 깊이, 넓이)
- O: regularization 추가
3. 2번 반복

## 전체 코드
```
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optimizer as optim

torch.manual_seed(1)
```

```
x_train = torch.FloatTensor([[1,2,3], [1,2,4], [1,3,4], [2,3,4], [1,5,6],[1,2,7],[1,6,6],[1,7,7]])
y_train = torch.LongTensor([2,2,2,1,1,1,0,0])
# |x_train|=(m,3), |y_train|=(m,)

x_test = torch.FloatTensor([[2,1,1],[3,1,2],[4,1,2]])
y_test = torch.LongTensor([2,2,2])
```

```
class SoftmaxClassifier(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(3,3) # (m,3) -> (m,3)
  
  def forward(self, x):
    return self.linear(x)

model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=0.1)
```

```
def train(model, optimizer, x_train, y_train):
  nb_epochs = 20
  for epoch in range(nb_epochs):
    prediction = model(x_train)
    cost = F.cross_entropy(prediction, y_train)
    # |x_train| = (m, 3), |prediction| = (m,3), |y_train| = (m,)

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
```

```
def test(model, optimizer, x_test, y_test):
  prediction = model(x_test)
  predicted_classes = prediction.max(1)[1]
  correct_count = (predicted_classes == y_test).sum().item()
  cost = F.cross_entropy(prediction, y_test)
```

## Learning Rate
- lr이 너무 크면 diverge해서 cost가 발산 (overshooting)
- lr이 너무 작으면 cost가 거의 줄어들지 않음
`cost를 보고 lr을 조절하는 것이 중요!`

## Data Preprocessing
ex) Standardization (정규분포)

![image](https://github.com/pomota/TIL/assets/132712212/56de585c-8784-43a9-bd5d-d2d6b7145734)
```
mu = x_train.mean(dim=0)
sigma = x_train.std(dim=0)
norm_x_train = (x_train-mu)/sigma
```
정규화를 수행하지 않을 경우

-> 모델이 큰 숫자에만 집중하게 되는 문제!
