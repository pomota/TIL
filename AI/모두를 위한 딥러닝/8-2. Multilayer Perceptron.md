## Multilayer Perceptron
### Backpropagation
multilayer perceptron을 학습하는 방법

![image](https://github.com/pomota/TIL/assets/132712212/fca9bfa2-02a8-4946-9bb1-e9619c2f78d5)

[backpropagation 엄밀한 증명] (https://jangpiano-science.tistory.com/125)

### XOR backpropagation 예제 코드
```
X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)
Y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)

w1 = torch.Tensor(2, 2).to(device)
b1 = torch.Tensor(2).to(device)
w2 = torch.Tensor(2, 1).to(device)
b2 = torch.Tensor(1).to(device)
# nn.Linear 두 층을 사용한 것과 같음

def sigmoid(x):
  return 1.0/(1.0+torch.exp(-x))

def sigmoid_prime(x):
  return sigmoid(x)*(1-sigmoid(x))
# sigmoid의 미분

for step in range(10001):
  # forward
  l1 = torch.add(torch.matmul(X, w1), b1)
  a1 = sigmoid(l1)

  l2 = torch.add(torch.matmul(a1, w2), b2)
  Y_pred = sigmoid(l2)

  # binary cross entropy
  cost = -torch.mean(Y * torch.log(Y_pred) + (1-Y) * torch.log(1-Y_pred))

  # back propagation (chain rule)
  # Loss derivative (마지막 항은 0으로 나누는 것 방지)
  d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)

  # Layer 2 미분
  d_l2 = d_Y_pred * sigmoid_prime(l2)
  d_b2 = d_l2
  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)
  # transpose(0, 1): 두개의 인덱스의 차원을 맞교환 (유사한 것으로 permute 함)

  # Layer 1 미분
  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))
  d_l1 = d_a1 * sigmoid_prime(l1)
  d_b1 = d_l1
  d_w2 = torch.matmul(torch.transpose(X, 0, 1), d_b1)

  # weight update
  w1 = w1 - learning_rate * d_w1
  b1 = b1 - learning_rate * torch.mean(d_b1, 0)
  w1 = w1 - learning_rate * d_w1
  b1 = b1 - learning_rate * torch.mean(d_b2, 0)
```
weight는 input neuron에 specific

bias는 neuron의 output에 input과 무관하게 영향을 줌

-> batch의 overall effect와 무관한 uniform update를 하기 위해 평균을 사용

### nn.module
```
X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)
Y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)

linear1 = torch.nn.Linear(2, 2, bias = True)
linear2 = torch.nn.Linear(2, 1, bias = True)
sigmoid = torch.nn.Sigmoid()
model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)

criterion = torch.nn.BCELoss().to(device)
optimizer = optim.SGD(model.parameters(), lr=1)
for step in range(10001):
  optimizer.zero_grad()
  hypothesis = model(X)
  cost = criterion(hypothesis, Y)
  cost.backward()
  optimizer.step()
  if step%100==0:
    print(step, cost.item())
```
